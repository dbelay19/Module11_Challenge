# Module 11 Challenge about web scrapping usig Id and calss
The gaol of this challneg was to help us apply what learned about extracting information via both automated browsing with Splinter and HTML parsing with Beautiful Soup. The module has two parts.
Part 1: Scrape Titles and Preview Text from Mars News:
*In this part of exceice, after setting import commands and read the url,  I have extracted titles and preview text of the news articles that that I have scraped. see example below:

![image](https://user-images.githubusercontent.com/117956888/223523053-323d8d1f-df41-46e7-a5d5-f07a37fdf30a.png)
![image](https://user-images.githubusercontent.com/117956888/223523208-ac7d63ce-b954-4350-8951-1900bef8e206.png)
![image](https://user-images.githubusercontent.com/117956888/223524293-805ecc6c-473f-49ae-8435-3ee042f380c4.png)


*Also exporetd the scraped data into a json file.
![image](https://user-images.githubusercontent.com/117956888/223523457-67cf2983-67d6-41dc-a679-087fc76fe38b.png)

Part 2: Scrape and Analyze Mars Weather Data: in this part of the challenge, I have created Create a Beautiful Soup object and used it to scrape the data in the HTML table. I have extracted information, put the data into a dataframe and show in graphs.

* Scrape the table:
![image](https://user-images.githubusercontent.com/117956888/223525311-64f00db7-d1ec-4b0f-aea5-afb037dad25e.png)


* store data into df:
* ![image](https://user-images.githubusercontent.com/117956888/223525455-bac0406e-255d-4e86-9a67-be2443af9fc4.png)
* Identify data types:
![image](https://user-images.githubusercontent.com/117956888/223525632-2e93013f-ea82-4eee-890c-31218f5c2cf0.png)

* Analize the data:
* ![image](https://user-images.githubusercontent.com/117956888/223525826-cec1ec8b-eab6-4ee6-88e8-bbc102761287.png)

* Display the data:
*
![image](https://user-images.githubusercontent.com/117956888/223525945-1d6cee9d-c4a7-477c-9006-786341ab9eba.png)

![image](https://user-images.githubusercontent.com/117956888/223526264-589f9bc7-eb5c-4fe3-986b-5780e4d51fcb.png)

* Finally save the data to a CSV file:
* ![image](https://user-images.githubusercontent.com/117956888/223526433-8b0ba5f2-58fc-48bd-8896-311922910477.png)




